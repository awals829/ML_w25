{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIY Multilayer Perceptron\n",
    "I've written some (questionable) code in [mlp-regressor.py](mlp-regressor.py) to try to implement a multi-layer perceptron. Let's use it to see if we can predict wine quality.\n",
    "\n",
    "First, you'll need to `pip install ucimlrepo` to get the data-fetching module. The `requirements.txt` file should now be fixed as well (it had some sneaky windows stuff in there)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from https://archive.ics.uci.edu/dataset/186/wine+quality\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split # lazy splitting\n",
    "  \n",
    "# fetch dataset \n",
    "wine_quality = fetch_ucirepo(id=186) \n",
    "  \n",
    "# data (as numpy arrays)\n",
    "X = wine_quality.data.features.values\n",
    "y = wine_quality.data.targets.values\n",
    "\n",
    "# print out the descriptors\n",
    "wine_quality.data.original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the original data\n",
    "import pandas as pd\n",
    "pd.plotting.scatter_matrix(wine_quality.data.original, alpha=0.3, figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the colour as binary encoded\n",
    "X = np.column_stack([X, (wine_quality.data.original.color == \"red\").astype(int)])\n",
    "\n",
    "# add some jitter to y\n",
    "y = y + np.random.normal(0, 0.1, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into 80/20/20 Train/test/val\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=219)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2/0.8, random_state=219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate norm params on test, excluding colour\n",
    "mu = X_train[:, :-1].mean(axis=0)\n",
    "std = X_train[:, :-1].std(axis=0)\n",
    "\n",
    "X_train[:, :-1] = (X_train[:, :-1] - mu) / std\n",
    "X_val[:, :-1] = (X_val[:, :-1] - mu) / std\n",
    "X_test[:, :-1] = (X_test[:, :-1] - mu) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train an MLP\n",
    "Up to now it's been pretty standard data exploration, preprocessing and splitting. You're welcome to tweak those things, of course.\n",
    "\n",
    "I've written a class `MLPRegressor` that should be able to train a multi-layer perceptron. It's not very good, but it's a start. You can use it like this:\n",
    "\n",
    "```python\n",
    "from mlp_regressor import MLPRegressor\n",
    "mlp = MLPRegressor(X_train.shape[1])\n",
    "mlp.add_layer(<number of neurons>, \"activation function\")\n",
    "... repeat\n",
    "print(mlp) # to see a summary of layers\n",
    "\n",
    "loss = mlp.train(X_train, y_train, step_size, epochs)\n",
    "plt.plot(loss)\n",
    "```\n",
    "\n",
    "It's very inefficient, so don't go too crazy with number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a basic MLP\n",
    "import importlib\n",
    "import mlp_regressor\n",
    "# importlib reloads the module from scratch every time you run this cell\n",
    "# useful if you make any changes to MLPRegressor\n",
    "importlib.reload(mlp_regressor)\n",
    "from mlp_regressor import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a scatter plot to check performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Modify the MLP\n",
    "Right now, the MLP only does whole-batch gradient descent. Modify it to allow training in batches. Does this help the performance?\n",
    "\n",
    "Try to read through the forward and backward passes to understand how it works. It's entirely possible I've made a mistake somewhere, so don't hesitate to ask if something doesn't make sense."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
